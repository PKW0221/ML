{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPkhTx98dsXLCnssr82pmPA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Importing dataset from sklearn\n","\n","from sklearn import datasets\n","from sklearn import metrics\n","\n","iris = datasets.load_breast_cancer() #dataset loading\n","X = iris.data               #Features stored in X \n","y = iris.target             #Class variable\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Installing xgboost\n","!pip install xgboost\n","\n","from xgboost import XGBClassifier\n","\n","clf = XGBClassifier()\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","\n","print(metrics.classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJVgpwaaaicU","executionInfo":{"status":"ok","timestamp":1665665765247,"user_tz":-540,"elapsed":5196,"user":{"displayName":"박경원","userId":"15636344126341059865"}},"outputId":"d6ca5d5b-124c-4ac2-b3cd-96ade7c2ba7a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.93      0.95        43\n","           1       0.96      0.99      0.97        71\n","\n","    accuracy                           0.96       114\n","   macro avg       0.97      0.96      0.96       114\n","weighted avg       0.97      0.96      0.96       114\n","\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"w9BikAkNziwQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665665768813,"user_tz":-540,"elapsed":17,"user":{"displayName":"박경원","userId":"15636344126341059865"}},"outputId":"a0869a0e-eaa1-4c8e-ee8a-69bcfc8a8a97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0])"]},"metadata":{},"execution_count":4}],"source":["# 이름과 학번을 작성해주세요\n","__author__ = \"박경원\"\n","__id__ = \"2017311362\"\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeRegressor\n","\n","\n","class my_XGB():\n","    def __init__(self):\n","        \"\"\"\n","        이 함수는 수정할 필요가 없습니다.\n","        \"\"\"\n","\n","        self.author = __author__\n","        self.id = __id__\n","\n","        self.boosting_N = 5\n","        self.weak_classifier = []\n","        self.score = None\n","        self.learning_rate = 0.4\n","        \n","\n","    def sigmoid(self, x):\n","        \"\"\"\n","        주어진 x에 대하여 sigmoid 활성화 함수를 코딩하세요.\n","        단, 파이썬 패키지인 math를 불러와서 사용할 수 없습니다. numpy만을 사용하여 코딩하세요.\n","        \"\"\"\n","        #문제 1: x에 대한 sigmoid 결과 값을 return 하세요. \n","        return 1 / (1 + np.exp(-x))        # 시그모이드 함수 공식 : 1/(1+exp(-x)) = exp(x)/(exp(x)+1)\n","\n","    def first_order(self, y, x):\n","        \"\"\"\n","        이 함수는 수정할 필요가 없습니다.\n","        \n","        본 함수는 Gradient를 연산하는 함수이며, loss를 근사하기 위해 사용됩니다.\n","        \"\"\"\n","        return (y - self.sigmoid(x))\n","\n","    def second_order(self, x):\n","        \"\"\"\n","        이 함수는 수정할 필요가 없습니다.\n","        \n","        본 함수는 Hessian을 연산하는 함수이며, 앞선 gradient를 미분한 값입니다.\n","        2차 근사함으로써 더욱 loss를 최소화하기 쉽게 합니다.\n","        \"\"\"\n","        return (self.sigmoid(x) * (1 - self.sigmoid(x)))\n","        \n","\n","    def fit(self, X_train, y_train):\n","        \"\"\"\n","        본 함수는 X_train, y_train를 활용하여 훈련하는 과정을 코딩하는 부분입니다.\n","\n","            설정한 boosting 횟수인 self.boosting_N만큼 훈련을 반복합니다.\n","            매 반복마다 임의의 decision tree 생성 한 후, 설정한 가중치를 기반으로 모델을 학습하고,\n","            학습된 Classifier는 self.weak_classifier 리스트에 저장되며,\n","            해당 Classifier의 예측 값은 self.learning rate를 반영하여 기존의 self.score와 합산됩니다.\n","        \n","        \"\"\"\n","        self.score = np.zeros(X_train.shape[0]) # 점수 초기화\n","        sample_weights = [1/len(y_train) for i in range(len(y_train))] # 임의의 가중치 할당\n","        y_new = y_train\n","        \n","        for i in range(self.boosting_N):\n","            # 임의의 decision tree 생성 한 후, 설정한 가중치를 기반으로 모델 학습\n","            # 유연한 가중치 부여 및 분류를 위해 regressor 호출\n","            # 학습된 Classifier는 self.weak_classifier 리스트에 저장\n","            new_dt = DecisionTreeRegressor()\n","            new_dt.fit(X_train, y_new, sample_weight=sample_weights) \n","            self.weak_classifier.append(new_dt) \n","            \n","            #문제 2: 방금 학습된 DT모델의 X_train 데이터에 대한 prediction score를 도출하세요.\n","            new_score = new_dt.predict(X_train)     \n","            # 앞서 만든 DecisionTreeRegressor() 를 이용하여 도출\n","\n","            #문제 3: 방금 구한 new_score에 learning rate를 반영(곱 연산)하여 self.score를 업데이트(합 연산)하세요.\n","            self.score = self.score + (1+self.learning_rate) * new_score      \n","            # 곱연산은 각 수에 1을 더해서 곱하고, 합연산에 의해 각 수를 다 더해준다.     \n","\n","            #문제 4: 앞선 모델의 gradient를 도출하기 위해 None 부분에 알맞은 변수를 입력하세요.\n","            grad = self.first_order(y_train, self.score)     \n","\n","            #문제 5: 앞선 모델의 hessian을 도출하기 위해 None 부분에 알맞은 변수를 입력하세요.\n","            hess = self.second_order(self.score)       \n","\n","            # gradient와 hessian을 반영하여 y_new와 가중치 업데이트\n","            y_new = grad/hess\n","            sample_weights = hess\n","\n","    def predict(self, X_test):\n","        \"\"\"\n","        본 함수는 X_test가 주어졌을 때, fit을 통해 계산된 classifier 리스트를 불러와 각 모델의 예측값을 합산하여 최종 도출하는 함수입니다.\n","        \"\"\"\n","        pred = np.zeros(X_test.shape[0])\n","        #문제 6: fit에서 저장된 weak_classifier들을 불러와 learning rate를 반영(곱 연산)한 prediction 값(합 연산)을 업데이트하세요.\n","        for w_classifier in self.weak_classifier:\n","            pred = pred + (1+self.learning_rate)*w_classifier.predict(X_test)\n","        # learning_rate를 곱 연산한 후 predict 값을 pred에 더하여 값을 구한다.\n","        \n","        probas = self.sigmoid(np.full((X_test.shape[0], 1), 1).flatten().astype('float64') + pred)\n","        #문제 7: probability가 전체 probability 평균보다 초과되면 1, 아닌경우 0으로 prediction 라벨을 할당하세요.\n","        pred_label = np.where(probas >  np.mean(probas),1,0) #np조건문을 사용하여 평균보다 크면 1, 작으면 0으로 할당 \n","        return pred_label\n","\n","\n","    def get_accuracy(self, y_true, y_pred):\n","        \"\"\"\n","        본 함수는 수정할 필요 없는 함수입니다.\n","        정확도를 연산하기 위한 함수입니다.\n","        \"\"\"\n","        return np.count_nonzero(y_true == y_pred) / len(y_true)\n","\n","    def score(self, x, y):\n","        \"\"\"\n","        본 함수는 수정할 필요 없는 함수입니다.\n","        5fold cross validation 함수를 사용하기 위해 필요한 함수입니다.\n","        \"\"\"\n","        y_pred = self.predict(x)\n","        return self.get_accuracy(y, y_pred)\n","\n","\n","    def get_params(self):\n","        \"\"\"\n","        본 함수는 수정할 필요 없는 함수입니다.\n","        5fold-cross validation을 수행하기 위하여 필요한 함수입니다.\n","        \"\"\"\n","        return {\n","            \"n_iters\":self.n_iters,\n","            \"lr\" :self.lr,\n","            \"lambda_param\" :self.lambda_param,\n","            \"random_seed\" :self.random_seed\n","            }\n"]}]}